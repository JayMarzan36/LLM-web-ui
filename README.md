# LLM-web-ui
I want to make my own version of OpenWebUI were I will host Ollama in a docker container and then interact with LLM models through this web ui.
